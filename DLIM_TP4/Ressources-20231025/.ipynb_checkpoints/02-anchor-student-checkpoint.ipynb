{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f86afb7",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Anchor Boxes\n",
    "\n",
    "\n",
    "Object detection algorithms usually\n",
    "sample a large number of regions in the input image, determine whether these regions contain\n",
    "objects of interest, and adjust the boundaries\n",
    "of the regions so as to predict the\n",
    "*ground-truth bounding boxes*\n",
    "of the objects more accurately.\n",
    "Different models may adopt\n",
    "different region sampling schemes. \n",
    "Here we introduce one of such methods:\n",
    "it generates multiple bounding boxes with varying scales and aspect ratios centered on each pixel. \n",
    "These bounding boxes are called *anchor boxes*.\n",
    "We will design an object detection model\n",
    "based on anchor boxes in [Part 5](./05-ssd-student.ipynb).\n",
    "\n",
    "First, let's modify the printing accuracy\n",
    "just for more concise outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32799c",
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from d2l import torch as d2l\n",
    "torch.set_printoptions(2)  # Simplify printing accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c999291",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "## Generating Multiple Anchor Boxes\n",
    "\n",
    "Suppose that the input image has a height of $h$ and width of $w$. \n",
    "We generate anchor boxes with different shapes centered on each pixel of the image.\n",
    "Let the *scale* be $s\\in (0, 1]$ and\n",
    "the *aspect ratio* (ratio of width to height) is $r > 0$. \n",
    "Then **the width and height of the anchor box are $ws\\sqrt{r}$ and $hs/\\sqrt{r}$, respectively.**\n",
    "Note that when the center position is given, an anchor box with known width and height is determined.\n",
    "\n",
    "To generate multiple anchor boxes with different shapes,\n",
    "let's set a series of scales\n",
    "$s_1,\\ldots, s_n$ and \n",
    "a series of aspect ratios $r_1,\\ldots, r_m$.\n",
    "When using all the combinations of these scales and aspect ratios with each pixel as the center,\n",
    "the input image will have a total of $whnm$ anchor boxes. Although these anchor boxes may cover all the\n",
    "ground-truth bounding boxes, the computational complexity is easily too high.\n",
    "In practice,\n",
    "we can only **consider those combinations\n",
    "containing** $s_1$ or $r_1$:\n",
    "\n",
    "$$(s_1, r_1), (s_1, r_2), \\ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \\ldots, (s_n, r_1).$$\n",
    "\n",
    "That is to say, the number of anchor boxes centered on the same pixel is $n+m-1$. For the entire input image, we will generate a total of $wh(n+m-1)$ anchor boxes.\n",
    "\n",
    "The above method of generating anchor boxes is implemented in the `multibox_prior` function. We specify the input image, a list of scales, and a list of aspect ratios, then this function will return all the anchor boxes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88c0db9a",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "\n",
    "> Experiment with the `multibox_prior` function and implement a simple, naive version in `multibox_prior_slow` below by completing its missing parts.\n",
    "> Once you have implemented it, you can check the [efficient implementation](https://github.com/d2l-ai/d2l-en/blob/e4f0ed18d9d83cd35434404daaf7ffc9631b5ece/d2l/torch.py#L1567) your can use with `d2l.multibox_prior`.\n",
    "\n",
    "*Hints:*\n",
    "- Output boxes are in the format `(x1, y1, x2, y2)` (\"two-corners\").\n",
    "- All coordinates are scaled to $[0,1]$ using image width and height.\n",
    "- Add $0.5$ to pixel coordinates before scaling their coordinates to point to the pixel center.\n",
    "- Why do we need to subtract $1$ in $(n + m -1)$? → To avoid generating the box $(s_1, r_1)$ twice!\n",
    "- The shape of the output must be `(1, num boxes, 4)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def multibox_prior_slow(data, sizes, ratios):\n",
    "    in_height, in_width = data.shape[-2:]\n",
    "    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)\n",
    "    boxes_per_pixel = (num_sizes + num_ratios - 1)\n",
    "    bboxes = []\n",
    "\n",
    "    def gen_bbox_xyxy(cx, cy, s, r, w_img, h_img):\n",
    "        cx_ = ????  # FIXME shifter and scaled center coordinates\n",
    "        cy_ = ????\n",
    "        w = ????  # FIXME scaled width and height\n",
    "        h = ????\n",
    "        x1 = ????  # FIXME final coordinates\n",
    "        x2 = ????\n",
    "        y1 = ????\n",
    "        y2 = ????\n",
    "        return torch.tensor((x1, y1, x2, y2))\n",
    "\n",
    "    for cx in ????:\n",
    "        for cy in ????:\n",
    "            for s in sizes:\n",
    "                r = ????  # Only the first ratio\n",
    "                bboxes.append(gen_bbox_xyxy(cx, cy, s, r, in_width, in_height))\n",
    "            for r in ????:  # Skip first ratio\n",
    "                s = ????  # Only the first size\n",
    "                bboxes.append(gen_bbox_xyxy(cx, cy, s, r, in_width, in_height))\n",
    "    output = torch.stack(bboxes, dim=0)\n",
    "    return output.reshape((1, ???? , 4))  # FIXME what is the output shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e3b93",
   "metadata": {
    "tags": [
     "TEACHER"
    ]
   },
   "outputs": [],
   "source": [
    "# TEACHER\n",
    "from math import sqrt\n",
    "def multibox_prior_slow(data, sizes, ratios):\n",
    "    in_height, in_width = data.shape[-2:]\n",
    "    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)\n",
    "    boxes_per_pixel = (num_sizes + num_ratios - 1)\n",
    "    # size_tensor = torch.tensor(sizes, device=device)\n",
    "    # ratio_tensor = torch.tensor(ratios, device=device)\n",
    "    bboxes = []\n",
    "\n",
    "    def gen_bbox_xyxy(cx, cy, s, r, w_img, h_img):\n",
    "        cx_ = (cx + 0.5) / w_img\n",
    "        cy_ = (cy + 0.5) / h_img\n",
    "        w = 1.0 * s * sqrt(r)\n",
    "        h = 1.0 * s / sqrt(r)\n",
    "        x1 = cx_ - (w / 2)\n",
    "        x2 = cx_ + (w / 2)\n",
    "        y1 = cy_ - (h / 2)\n",
    "        y2 = cy_ + (h / 2)\n",
    "        return torch.tensor((x1, y1, x2, y2))\n",
    "\n",
    "    for cx in range(in_width):\n",
    "        for cy in range(in_height):\n",
    "            for s in sizes:\n",
    "                r = ratios[0]\n",
    "                bboxes.append(gen_bbox_xyxy(cx, cy, s, r, in_width, in_height))\n",
    "            for r in ratios[1:]:\n",
    "                s = sizes[0]\n",
    "                bboxes.append(gen_bbox_xyxy(cx, cy, s, r, in_width, in_height))\n",
    "    output = torch.stack(bboxes, dim=0)\n",
    "    return output.reshape((1, boxes_per_pixel * in_height * in_width, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e2b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt1 = multibox_prior_slow(torch.zeros(size=(1, 3, 3, 3)), [0.2], [0.5, 1, 2])\n",
    "print(tt1.shape)\n",
    "tt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5829be",
   "metadata": {},
   "outputs": [],
   "source": [
    "multibox_prior = d2l.multibox_prior"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74687c27",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "We can see that **the shape of the returned anchor box variable `Y`** is\n",
    "(batch size, number of anchor boxes, 4).\n",
    "\n",
    "\n",
    "This could be reshaped to (batch size, w, h, number of boxes per pixel, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ac0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = multibox_prior(torch.zeros(size=(1, 3, 3, 3)), [0.2], [0.5, 1, 2])\n",
    "print(tt.shape)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b23517",
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "!test -f catdog.jpg || wget https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/catdog.jpg\n",
    "\n",
    "img = plt.imread('./catdog.jpg')\n",
    "h, w = img.shape[:2]\n",
    "print(h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = torch.zeros(size=(1, 3, h, w))  # Construct input data\n",
    "Y = multibox_prior_slow(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X = torch.zeros(size=(1, 3, h, w))  # Construct input data\n",
    "Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d69f60",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "After changing the shape of the anchor box variable `Y` to (image height, image width, number of anchor boxes centered on the same pixel, 4),\n",
    "we can obtain all the anchor boxes centered on a specified pixel position.\n",
    "In the following,\n",
    "we **access the first anchor box centered on (250, 250)**. It has four elements: the $(x, y)$-axis coordinates at the upper-left corner and the $(x, y)$-axis coordinates at the lower-right corner of the anchor box.\n",
    "The coordinate values of both axes\n",
    "are divided by the width and height of the image, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be1c26",
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "boxes = Y.reshape(h, w, 5, 4)\n",
    "boxes[250, 250, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883bec3",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "In order to **show all the anchor boxes centered on one pixel in the image**,\n",
    "we import the `show_bboxes` function to draw multiple bounding boxes on the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b87eef3",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "show_bboxes = d2l.show_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb9ae2",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "As we just saw, the coordinate values of the $x$ and $y$ axes in the variable `boxes` have been divided by the width and height of the image, respectively.\n",
    "When drawing anchor boxes,\n",
    "we need to restore their original coordinate values;\n",
    "thus, we define variable `bbox_scale` below. \n",
    "Now, we can draw all the anchor boxes centered on (250, 250) in the image.\n",
    "As you can see, the blue anchor box with a scale of 0.75 and an aspect ratio of 1 well\n",
    "surrounds the dog in the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e0576",
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "bbox_scale = torch.tensor((w, h, w, h))\n",
    "fig = plt.imshow(img)\n",
    "show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,\n",
    "            ['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2',\n",
    "             's=0.75, r=0.5'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39ba4550",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "## **Intersection over Union (IoU)**\n",
    "\n",
    "We just mentioned that an anchor box \"well\" surrounds the dog in the image.\n",
    "If the ground-truth bounding box of the object is known, how can \"well\" here be quantified?\n",
    "Intuitively, we can measure the similarity between\n",
    "the anchor box and the ground-truth bounding box.\n",
    "We know that the *Jaccard index* can measure the similarity between two sets. Given sets $\\mathcal{A}$ and $\\mathcal{B}$, their Jaccard index is the size of their intersection divided by the size of their union:\n",
    "\n",
    "$$J(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}.$$\n",
    "\n",
    "\n",
    "In fact, we can consider the pixel area of any bounding box as a set of pixels. \n",
    "In this way, we can measure the similarity of the two bounding boxes by the Jaccard index of their pixel sets. For two bounding boxes, we usually refer their Jaccard index as *intersection over union* (*IoU*), which is the ratio of their intersection area to their union area, as shown in the figure below.\n",
    "The range of an IoU is between 0 and 1:\n",
    "0 means that two bounding boxes do not overlap at all,\n",
    "while 1 indicates that the two bounding boxes are equal.\n",
    "\n",
    "![IoU is the ratio of the intersection area to the union area of two bounding boxes.](https://raw.githubusercontent.com/d2l-ai/d2l-en/e4f0ed18d9d83cd35434404daaf7ffc9631b5ece/img/iou.svg)\n",
    "\n",
    "For the remainder of this section, we will use IoU to measure the similarity between anchor boxes and ground-truth bounding boxes, and between different anchor boxes.\n",
    "Given two lists of anchor or bounding boxes,\n",
    "the following `box_iou` computes their pairwise IoU\n",
    "across these two lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00639889",
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def box_iou(boxes1, boxes2):\n",
    "    \"\"\"Compute pairwise IoU across two lists of anchor or bounding boxes.\"\"\"\n",
    "    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n",
    "                              (boxes[:, 3] - boxes[:, 1]))\n",
    "    # Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),\n",
    "    # (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)\n",
    "    areas1 = box_area(boxes1)\n",
    "    areas2 = box_area(boxes2)\n",
    "    # Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of\n",
    "    # boxes1, no. of boxes2, 2)\n",
    "    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # beautiful broadcasting trick\n",
    "    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)\n",
    "    # Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)\n",
    "    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n",
    "    union_areas = areas1[:, None] + areas2 - inter_areas\n",
    "    return inter_areas / union_areas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dce93a68",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "## Labeling Anchor Boxes in Training Data\n",
    "In a training dataset,\n",
    "we consider each anchor box as a training example.\n",
    "In order to train an object detection model,\n",
    "we need *class* and *offset* labels for each anchor box,\n",
    "where the former is\n",
    "the class of the object relevant to the anchor box\n",
    "and the latter is the offset\n",
    "of the ground-truth bounding box relative to the anchor box.\n",
    "During the prediction,\n",
    "for each image\n",
    "we generate multiple anchor boxes,\n",
    "predict classes and offsets for all the anchor boxes,\n",
    "adjust their positions according to the predicted offsets to obtain the predicted bounding boxes,\n",
    "and finally only output those \n",
    "predicted bounding boxes that satisfy certain criteria.\n",
    "\n",
    "\n",
    "As we know, an object detection training set\n",
    "comes with labels for\n",
    "locations of *ground-truth bounding boxes*\n",
    "and classes of their surrounded objects.\n",
    "To label any generated *anchor box*,\n",
    "we refer to the labeled\n",
    "location and class of its *assigned* ground-truth bounding box that is closest to the anchor box.\n",
    "In the following,\n",
    "we describe an algorithm for assigning\n",
    "closest ground-truth bounding boxes to anchor boxes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993ec3f",
   "metadata": {},
   "source": [
    "\n",
    "### **Assigning Ground-Truth Bounding Boxes to Anchor Boxes**\n",
    "\n",
    "Given an image,\n",
    "suppose that the anchor boxes are $A_1, A_2, \\ldots, A_{n_a}$ and the ground-truth bounding boxes are $B_1, B_2, \\ldots, B_{n_b}$, where $n_a \\geq n_b$.\n",
    "Let's define a matrix $\\mathbf{X} \\in \\mathbb{R}^{n_a \\times n_b}$, whose element $x_{ij}$ in the $i^\\textrm{th}$ row and $j^\\textrm{th}$ column is the IoU of the anchor box $A_i$ and the ground-truth bounding box $B_j$. The algorithm consists of the following steps:\n",
    "\n",
    "1. Find the largest element in matrix $\\mathbf{X}$ and denote its row and column indices as $i_1$ and $j_1$, respectively. Then the ground-truth bounding box $B_{j_1}$ is assigned to the anchor box $A_{i_1}$. This is quite intuitive because $A_{i_1}$ and $B_{j_1}$ are the closest among all the pairs of anchor boxes and ground-truth bounding boxes. After the first assignment, discard all the elements in the ${i_1}^\\textrm{th}$ row and the ${j_1}^\\textrm{th}$ column in matrix $\\mathbf{X}$. \n",
    "1. Find the largest of the remaining elements in matrix $\\mathbf{X}$ and denote its row and column indices as $i_2$ and $j_2$, respectively. We assign ground-truth bounding box $B_{j_2}$ to anchor box $A_{i_2}$ and discard all the elements in the ${i_2}^\\textrm{th}$ row and the ${j_2}^\\textrm{th}$ column in matrix $\\mathbf{X}$.\n",
    "1. At this point, elements in two rows and two columns in  matrix $\\mathbf{X}$ have been discarded. We proceed until all elements in $n_b$ columns in matrix $\\mathbf{X}$ are discarded. At this time, we have assigned a ground-truth bounding box to each of $n_b$ anchor boxes.\n",
    "1. Only traverse through the remaining $n_a - n_b$ anchor boxes. For example, given any anchor box $A_i$, find the ground-truth bounding box $B_j$ with the largest IoU with $A_i$ throughout the $i^\\textrm{th}$ row of matrix $\\mathbf{X}$, and assign $B_j$ to $A_i$ only if this IoU is greater than a predefined threshold.\n",
    "\n",
    "Let's illustrate the above algorithm using a concrete\n",
    "example.\n",
    "As shown in the figure below (left), assuming that the maximum value in matrix $\\mathbf{X}$ is $x_{23}$, we assign the ground-truth bounding box $B_3$ to the anchor box $A_2$.\n",
    "Then, we discard all the elements in row 2 and column 3 of the matrix, find the largest $x_{71}$ in the remaining elements (shaded area), and assign the ground-truth bounding box $B_1$ to the anchor box $A_7$. \n",
    "Next, as shown in the figure below (middle), discard all the elements in row 7 and column 1 of the matrix, find the largest $x_{54}$ in the remaining elements (shaded area), and assign the ground-truth bounding box $B_4$ to the anchor box $A_5$. \n",
    "Finally, as shown in the figure below (right), discard all the elements in row 5 and column 4 of the matrix, find the largest $x_{92}$ in the remaining elements (shaded area), and assign the ground-truth bounding box $B_2$ to the anchor box $A_9$.\n",
    "After that, we only need to traverse through\n",
    "the remaining anchor boxes $A_1, A_3, A_4, A_6, A_8$ and determine whether to assign them ground-truth bounding boxes according to the threshold.\n",
    "\n",
    "![Assigning ground-truth bounding boxes to anchor boxes.](https://raw.githubusercontent.com/d2l-ai/d2l-en/e4f0ed18d9d83cd35434404daaf7ffc9631b5ece/img/anchor-label.svg)\n",
    "\n",
    "This algorithm is implemented in the `assign_anchor_to_bbox` function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d69d40",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "> Write a naive but correct implementation of the `assign_anchor_to_bbox_slow` function below by filling its missing parts.\n",
    "> Once you have finished, you can look at the [efficient implementation](https://github.com/d2l-ai/d2l-en/blob/e4f0ed18d9d83cd35434404daaf7ffc9631b5ece/d2l/torch.py#L1650).\n",
    "\n",
    "*Hints:*\n",
    "- Use `torch.full((num_elem,), -1, dtype=torch.long)` to assign `-1` to a complete row or column.\n",
    "- Use `torch.argmax()` to get the index of the largest element in a tensor.\n",
    "- Only check the threshold in the part where you check remaining anchor boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c27e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_anchor_to_bbox_slow(ground_truth, anchors, iou_threshold=0.5):\n",
    "    \"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\n",
    "    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n",
    "    # Element x_ij in the i-th row and j-th column is the IoU of the anchor\n",
    "    # box i and the ground-truth bounding box j\n",
    "    jaccard = box_iou(anchors, ground_truth)\n",
    "    # print(jaccard)  # you can look at it if you want!\n",
    "    # Initialize the tensor to hold the assigned ground-truth bounding box for\n",
    "    # each anchor\n",
    "    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long)\n",
    "    \n",
    "    # In practice, we must perform the last part first to avoid erasing the jaccard matrix\n",
    "    # Traverse through the unassigned anchors, and assign them\n",
    "    # the best-matching target bbox, if their IoU is large enough\n",
    "    for anc_i in range(num_anchors):\n",
    "        ????\n",
    "\n",
    "    # In practice, we perform this step last\n",
    "    # the goal is the ensure the best possible coupling between anchors and targets\n",
    "    # assign each target bbox to an anchor\n",
    "    for box_j in range(num_gt_boxes):\n",
    "        ????\n",
    "    \n",
    "    return anchors_bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc586545",
   "metadata": {
    "tags": [
     "TEACHER"
    ]
   },
   "outputs": [],
   "source": [
    "# TEACHER\n",
    "def assign_anchor_to_bbox_slow(ground_truth, anchors, iou_threshold=0.5):\n",
    "    \"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\n",
    "    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n",
    "    # Element x_ij in the i-th row and j-th column is the IoU of the anchor\n",
    "    # box i and the ground-truth bounding box j\n",
    "    jaccard = box_iou(anchors, ground_truth)\n",
    "    # print(jaccard)  # you can look at it if you want!\n",
    "    # Initialize the tensor to hold the assigned ground-truth bounding box for\n",
    "    # each anchor\n",
    "    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long)\n",
    "    \n",
    "    # In practice, we must perform the last part first to avoid erasing the jaccard matrix\n",
    "    # Traverse through the unassigned anchors, and assign them\n",
    "    # the best-matching target bbox, if their IoU is large enough\n",
    "    for anc_i in range(num_anchors):\n",
    "        # consider only unassigned anchors\n",
    "        # if anchors_bbox_map[anc_i] > -1:\n",
    "        #     continue\n",
    "        # find best match\n",
    "        # we would have erased jaccard if we this this second\n",
    "        box_j = torch.argmax(jaccard[anc_i,:])\n",
    "        # check IoU\n",
    "        if jaccard[anc_i, box_j] > iou_threshold:\n",
    "            # assign box_j → anc_i\n",
    "            anchors_bbox_map[anc_i] = box_j\n",
    "            # print(f\"pre-assign {box_j} → {anc_i}\")\n",
    "\n",
    "    # In practice, we perform this step last\n",
    "    # the goal is the ensure the best possible coupling between anchors and targets\n",
    "    # assign each target bbox to an anchor\n",
    "    for box_j in range(num_gt_boxes):\n",
    "        # find best match\n",
    "        anc_i = torch.argmax(jaccard[:,box_j])\n",
    "        # check IoU\n",
    "        # if jaccard[anc_i, box_j] > iou_threshold:\n",
    "        # assign box_j → anc_i\n",
    "        anchors_bbox_map[anc_i] = box_j\n",
    "        # print(f\"assign {box_j} → {anc_i}\")\n",
    "        # discard\n",
    "        jaccard[anc_i,:] = torch.full((num_gt_boxes,), -1, dtype=torch.long)\n",
    "        jaccard[:,box_j] = torch.full((num_anchors,), -1, dtype=torch.long)\n",
    "    \n",
    "    return anchors_bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_anchor_to_bbox = d2l.assign_anchor_to_bbox\n",
    "t_gt = torch.tensor([[0,0, 5, 5], [10, 10, 20, 20]])\n",
    "t_anc = torch.tensor([[1,0, 5, 5], [9, 9, 15, 15], [1, 2, 5, 2], [14, 14, 16, 16], [20, 20, 25, 25]])\n",
    "%time assign_anchor_to_bbox(t_gt, t_anc, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time assign_anchor_to_bbox_slow(t_gt, t_anc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bda576be",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "### Labeling Classes and Offsets\n",
    "\n",
    "Now we can label the class and offset for each anchor box. Suppose that an anchor box $A$ is assigned\n",
    "a ground-truth bounding box $B$. \n",
    "On the one hand,\n",
    "the class of the anchor box $A$ will be\n",
    "labeled as that of $B$.\n",
    "On the other hand,\n",
    "the offset of the anchor box $A$ \n",
    "will be labeled according to the \n",
    "relative position between\n",
    "the central coordinates of $B$ and $A$\n",
    "together with the relative size between\n",
    "these two boxes.\n",
    "Given varying\n",
    "positions and sizes of different boxes in the dataset,\n",
    "we can apply transformations\n",
    "to those relative positions and sizes\n",
    "that may lead to \n",
    "more uniformly distributed offsets\n",
    "that are easier to fit.\n",
    "Here we describe a common transformation.\n",
    "\n",
    "**Given the central coordinates of $A$ and $B$ as $(x_a, y_a)$ and $(x_b, y_b)$,**\n",
    "**their widths as $w_a$ and $w_b$,**\n",
    "**and their heights as $h_a$ and $h_b$, respectively.**\n",
    "**We may label the offset of $A$ as**\n",
    "\n",
    "$$\\left( \\frac{ \\frac{x_b - x_a}{w_a} - \\mu_x }{\\sigma_x},\n",
    "\\frac{ \\frac{y_b - y_a}{h_a} - \\mu_y }{\\sigma_y},\n",
    "\\frac{ \\log \\frac{w_b}{w_a} - \\mu_w }{\\sigma_w},\n",
    "\\frac{ \\log \\frac{h_b}{h_a} - \\mu_h }{\\sigma_h}\\right),$$\n",
    "\n",
    "where default values of the constants are $\\mu_x = \\mu_y = \\mu_w = \\mu_h = 0, \\sigma_x=\\sigma_y=0.1$, and $\\sigma_w=\\sigma_h=0.2$.\n",
    "This transformation is implemented below in the `offset_boxes` function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "209d95f3",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "> Implement the `offset_boxes_student` function below and compare its behavior to the expected one.\n",
    "> One you have finished, you can look at the [correct code](https://github.com/d2l-ai/d2l-en/blob/e4f0ed18d9d83cd35434404daaf7ffc9631b5ece/d2l/torch.py#L1678).\n",
    "\n",
    "**Important**\n",
    "\n",
    "- These class and offset are what the network will learn to predict for each box!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55167abf",
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "offset_boxes = d2l.offset_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f66cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_boxes_student(anchors, assigned_bb, eps=1e-6):\n",
    "    \"\"\"Transform for anchor box offsets.\"\"\"\n",
    "    c_anc = d2l.box_corner_to_center(anchors)\n",
    "    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)\n",
    "    xy = 10 * ...  # ??\n",
    "    wh = 5 * ...  # ??\n",
    "    offset = torch.cat((xy, wh), dim=1)\n",
    "    return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = torch.tensor([[0, 0, 2, 2], [1, 1, 3, 3]])\n",
    "assigned_bb = torch.tensor([[1, 1, 2.5, 2.5], [2, 2, 4, 4]])\n",
    "result = offset_boxes_student(anchors, assigned_bb)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_result = offset_boxes(anchors, assigned_bb)\n",
    "print(expected_result)\n",
    "assert torch.allclose(result, expected_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e70f1e3",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "If an anchor box is not assigned a ground-truth bounding box, we just label the class of the anchor box as \"background\".\n",
    "Anchor boxes whose classes are background are often referred to as *negative* anchor boxes,\n",
    "and the rest are called *positive* anchor boxes.\n",
    "We implement the following `multibox_target` function\n",
    "to **label classes and offsets for anchor boxes** (the `anchors` argument) using ground-truth bounding boxes (the `labels` argument).\n",
    "This function sets the background class to zero and increments the integer index of a new class by one (because the class numbering starts at 0 in the dataset's ground truth).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90367eb0",
   "metadata": {
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def multibox_target(anchors, labels):\n",
    "    # labels.shape: (batch size, 5) (5 = 1 class id + 4 coordinates)\n",
    "    \"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\n",
    "    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n",
    "    batch_offset, batch_mask, batch_class_labels = [], [], []\n",
    "    device, num_anchors = anchors.device, anchors.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        label = labels[i, :, :]\n",
    "        anchors_bbox_map = assign_anchor_to_bbox(\n",
    "            label[:, 1:], anchors, device)\n",
    "        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(\n",
    "            1, 4)\n",
    "        # Initialize class labels and assigned bounding box coordinates with\n",
    "        # zeros\n",
    "        class_labels = torch.zeros(num_anchors, dtype=torch.long,\n",
    "                                   device=device)\n",
    "        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,\n",
    "                                  device=device)\n",
    "        # Label classes of anchor boxes using their assigned ground-truth\n",
    "        # bounding boxes. If an anchor box is not assigned any, we label its\n",
    "        # class as background (the value remains zero)\n",
    "        indices_true = torch.nonzero(anchors_bbox_map >= 0)  # (note): faster masking using indices\n",
    "        bb_idx = anchors_bbox_map[indices_true]\n",
    "        class_labels[indices_true] = label[bb_idx, 0].long() + 1\n",
    "        assigned_bb[indices_true] = label[bb_idx, 1:]\n",
    "        # Offset transformation\n",
    "        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\n",
    "        batch_offset.append(offset.reshape(-1))\n",
    "        batch_mask.append(bbox_mask.reshape(-1))\n",
    "        batch_class_labels.append(class_labels)\n",
    "    bbox_offset = torch.stack(batch_offset)\n",
    "    bbox_mask = torch.stack(batch_mask)\n",
    "    class_labels = torch.stack(batch_class_labels)\n",
    "    return (bbox_offset, bbox_mask, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb37ac7f",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "### An Example\n",
    "\n",
    "Let's illustrate anchor box labeling\n",
    "via a concrete example.\n",
    "We define ground-truth bounding boxes for the dog and cat in the loaded image,\n",
    "where the first element is the class (0 for dog and 1 for cat) and the remaining four elements are the\n",
    "$(x, y)$-axis coordinates\n",
    "at the upper-left corner and the lower-right corner\n",
    "(range is between 0 and 1). \n",
    "We also construct five anchor boxes to be labeled\n",
    "using the coordinates of\n",
    "the upper-left corner and the lower-right corner:\n",
    "$A_0, \\ldots, A_4$ (the index starts from 0).\n",
    "Then we **plot these ground-truth bounding boxes \n",
    "and anchor boxes \n",
    "in the image.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc697a",
   "metadata": {
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "ground_truth = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],\n",
    "                         [1, 0.55, 0.2, 0.9, 0.88]])\n",
    "anchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],\n",
    "                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],\n",
    "                    [0.57, 0.3, 0.92, 0.9]])\n",
    "\n",
    "fig = plt.imshow(img)\n",
    "show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')\n",
    "show_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e868b1b3",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "Using the `multibox_target` function defined above,\n",
    "we can **label classes and offsets\n",
    "of these anchor boxes based on\n",
    "the ground-truth bounding boxes** for the dog and cat.\n",
    "In this example, indices of\n",
    "the background, dog, and cat classes\n",
    "are 0, 1, and 2, respectively. \n",
    "Below we add an dimension for examples of anchor boxes and ground-truth bounding boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc704230",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:58.754261Z",
     "iopub.status.busy": "2023-08-18T07:05:58.753960Z",
     "iopub.status.idle": "2023-08-18T07:05:58.760830Z",
     "shell.execute_reply": "2023-08-18T07:05:58.760042Z"
    },
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "labels = multibox_target(anchors.unsqueeze(dim=0),\n",
    "                         ground_truth.unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588c1b5",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "There are three items in the returned result, all of which are in the tensor format.\n",
    "The third item contains the labeled classes of the input anchor boxes.\n",
    "\n",
    "Let's analyze the returned class labels below based on\n",
    "anchor box and ground-truth bounding box positions in the image.\n",
    "First, among all the pairs of anchor boxes\n",
    "and ground-truth bounding boxes,\n",
    "the IoU of the anchor box $A_4$ and the ground-truth bounding box of the cat is the largest. \n",
    "Thus, the class of $A_4$ is labeled as the cat.\n",
    "Taking out \n",
    "pairs containing $A_4$ or the ground-truth bounding box of the cat, among the rest \n",
    "the pair of the anchor box $A_1$ and the ground-truth bounding box of the dog has the largest IoU.\n",
    "So the class of $A_1$ is labeled as the dog.\n",
    "Next, we need to traverse through the remaining three unlabeled anchor boxes: $A_0$, $A_2$, and $A_3$.\n",
    "For $A_0$,\n",
    "the class of the ground-truth bounding box with the largest IoU is the dog,\n",
    "but the IoU is below the predefined threshold (0.5),\n",
    "so the class is labeled as background;\n",
    "for $A_2$,\n",
    "the class of the ground-truth bounding box with the largest IoU is the cat and the IoU exceeds the threshold, so the class is labeled as the cat;\n",
    "for $A_3$,\n",
    "the class of the ground-truth bounding box with the largest IoU is the cat, but the value is below the threshold, so the class is labeled as background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cecdfb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:58.764032Z",
     "iopub.status.busy": "2023-08-18T07:05:58.763740Z",
     "iopub.status.idle": "2023-08-18T07:05:58.769539Z",
     "shell.execute_reply": "2023-08-18T07:05:58.768692Z"
    },
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "labels[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a708b",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "The second returned item is a mask variable of the shape (batch size, four times the number of anchor boxes).\n",
    "Every four elements in the mask variable \n",
    "correspond to the four offset values of each anchor box.\n",
    "Since we do not care about background detection,\n",
    "offsets of this negative class should not affect the objective function.\n",
    "Through elementwise multiplications, zeros in the mask variable will filter out negative class offsets before calculating the objective function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c5d346",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:58.772711Z",
     "iopub.status.busy": "2023-08-18T07:05:58.772421Z",
     "iopub.status.idle": "2023-08-18T07:05:58.778256Z",
     "shell.execute_reply": "2023-08-18T07:05:58.777478Z"
    },
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28e3344",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "The first returned item contains the four offset values labeled for each anchor box.\n",
    "Note that the offsets of negative-class anchor boxes are labeled as zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2eecec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:58.781418Z",
     "iopub.status.busy": "2023-08-18T07:05:58.781096Z",
     "iopub.status.idle": "2023-08-18T07:05:58.786779Z",
     "shell.execute_reply": "2023-08-18T07:05:58.786014Z"
    },
    "origin_pos": 36,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d437974b",
   "metadata": {
    "origin_pos": 37
   },
   "source": [
    "## Predicting Bounding Boxes with Non-Maximum Suppression\n",
    "\n",
    "During prediction,\n",
    "we generate multiple anchor boxes for the image and predict classes and offsets for each of them.\n",
    "A *predicted bounding box*\n",
    "is thus obtained according to \n",
    "an anchor box with its predicted offset.\n",
    "Below we implement the `offset_inverse` function\n",
    "that takes in anchors and\n",
    "offset predictions as inputs and **applies inverse offset transformations to\n",
    "return the predicted bounding box coordinates**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ccea2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:58.789937Z",
     "iopub.status.busy": "2023-08-18T07:05:58.789649Z",
     "iopub.status.idle": "2023-08-18T07:05:58.794637Z",
     "shell.execute_reply": "2023-08-18T07:05:58.793871Z"
    },
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def offset_inverse(anchors, offset_preds):\n",
    "    \"\"\"Predict bounding boxes based on anchor boxes with predicted offsets.\"\"\"\n",
    "    anc = d2l.box_corner_to_center(anchors)\n",
    "    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]\n",
    "    pred_bbox_wh = torch.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]\n",
    "    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), axis=1)\n",
    "    predicted_bbox = d2l.box_center_to_corner(pred_bbox)\n",
    "    return predicted_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e278c8",
   "metadata": {
    "origin_pos": 39
   },
   "source": [
    "When there are many anchor boxes,\n",
    "many similar (with significant overlap)\n",
    "predicted bounding boxes \n",
    "can be potentially output for surrounding the same object.\n",
    "To simplify the output,\n",
    "we can merge similar predicted bounding boxes\n",
    "that belong to the same object\n",
    "by using *non-maximum suppression* (NMS).\n",
    "\n",
    "Here is how non-maximum suppression works.\n",
    "For a predicted bounding box $B$,\n",
    "the object detection model calculates the predicted likelihood\n",
    "for each class.\n",
    "Denoting by $p$ the largest predicted likelihood,\n",
    "the class corresponding to this probability is the predicted class for $B$.\n",
    "Specifically, we refer to $p$ as the *confidence* (score) of the predicted bounding box $B$.\n",
    "On the same image,\n",
    "all the predicted non-background bounding boxes \n",
    "are sorted by confidence in descending order\n",
    "to generate a list $L$.\n",
    "Then we manipulate the sorted list $L$ in the following steps:\n",
    "\n",
    "1. Select the predicted bounding box $B_1$ with the highest confidence from $L$ as a basis and remove all non-basis predicted bounding boxes whose IoU with $B_1$ exceeds a predefined threshold $\\epsilon$ from $L$. At this point, $L$ keeps the predicted bounding box with the highest confidence but drops others that are too similar to it. In a nutshell, those with *non-maximum* confidence scores are *suppressed*.\n",
    "1. Select the predicted bounding box $B_2$ with the second highest confidence from $L$ as another basis and remove all non-basis predicted bounding boxes whose IoU with $B_2$ exceeds $\\epsilon$ from $L$.\n",
    "1. Repeat the above process until all the predicted bounding boxes in $L$ have been used as a basis. At this time, the IoU of any pair of predicted bounding boxes in $L$ is below the threshold $\\epsilon$; thus, no pair is too similar with each other. \n",
    "1. Output all the predicted bounding boxes in the list $L$.\n",
    "\n",
    "**The `nms` function sorts confidence scores in descending order and returns their indices.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ad261",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "> Complete the code of the function `nms_student` below and compare its behavior to the correct implementation.\n",
    "> Once you have finished, you can [check its code](https://github.com/d2l-ai/d2l-en/blob/e4f0ed18d9d83cd35434404daaf7ffc9631b5ece/d2l/torch.py#L1736)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec43c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:58.798695Z",
     "iopub.status.busy": "2023-08-18T07:05:58.798402Z",
     "iopub.status.idle": "2023-08-18T07:05:58.804081Z",
     "shell.execute_reply": "2023-08-18T07:05:58.803312Z"
    },
    "origin_pos": 41,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def nms_student(boxes, scores, iou_threshold):\n",
    "    \"\"\"Sort confidence scores of predicted bounding boxes.\"\"\"\n",
    "    B = torch.argsort(scores, dim=-1, descending=True)\n",
    "    keep = []  # Indices of predicted bounding boxes that will be kept\n",
    "    \n",
    "    while B.numel() > 0:\n",
    "        i = ????  # Keep the first indice, of the largest score\n",
    "        keep.append(i)\n",
    "\n",
    "        iou = box_iou(\n",
    "            boxes[???].reshape(-1,4),  # current boxe\n",
    "            boxes[????].reshape(-1,4)) # other boxes\n",
    "        \n",
    "        non_covered = torch.nonzero(iou ??? iou_threshold)  # Gather indices of boxes which are not overlapping\n",
    "        B = B[non_covered + 1]  # Update the sorted list of indexes to process\n",
    "\n",
    "    return torch.tensor(keep, device=boxes.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339729e",
   "metadata": {
    "tags": [
     "TEACHER"
    ]
   },
   "outputs": [],
   "source": [
    "# TEACHER\n",
    "def nms_student(boxes, scores, iou_threshold):\n",
    "    \"\"\"Sort confidence scores of predicted bounding boxes.\"\"\"\n",
    "    B = torch.argsort(scores, dim=-1, descending=True)\n",
    "    keep = []  # Indices of predicted bounding boxes that will be kept\n",
    "    \n",
    "    while B.numel() > 0:\n",
    "        i = B[0]\n",
    "        keep.append(i)\n",
    "\n",
    "        iou = box_iou(boxes[i].reshape(-1,4), boxes[B[1:]].reshape(-1,4))\n",
    "        non_covered = torch.nonzero(iou <= iou_threshold)\n",
    "        B = B[non_covered + 1]\n",
    "        \n",
    "    return torch.tensor(keep, device=boxes.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add some tests here\n",
    "nms = d2l.nms\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57c7c5",
   "metadata": {
    "origin_pos": 42
   },
   "source": [
    "We define the following `multibox_detection`\n",
    "to **apply non-maximum suppression\n",
    "to predicting bounding boxes**.\n",
    "Do not worry if you find the implementation\n",
    "a bit complicated: we will show how it works\n",
    "with a concrete example right after the implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c6d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:58.807439Z",
     "iopub.status.busy": "2023-08-18T07:05:58.807147Z",
     "iopub.status.idle": "2023-08-18T07:05:58.815505Z",
     "shell.execute_reply": "2023-08-18T07:05:58.814729Z"
    },
    "origin_pos": 44,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,\n",
    "                       pos_threshold=0.009999999):\n",
    "    \"\"\"Predict bounding boxes using non-maximum suppression.\"\"\"\n",
    "    device, batch_size = cls_probs.device, cls_probs.shape[0]\n",
    "    anchors = anchors.squeeze(0)\n",
    "    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]\n",
    "    out = []\n",
    "    for i in range(batch_size):\n",
    "        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)\n",
    "        conf, class_id = torch.max(cls_prob[1:], 0)\n",
    "        predicted_bb = offset_inverse(anchors, offset_pred)\n",
    "        keep = nms(predicted_bb, conf, nms_threshold)\n",
    "        # Find all non-`keep` indices and set the class to background\n",
    "        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)\n",
    "        combined = torch.cat((keep, all_idx))\n",
    "        uniques, counts = combined.unique(return_counts=True)\n",
    "        non_keep = uniques[counts == 1]\n",
    "        all_id_sorted = torch.cat((keep, non_keep))\n",
    "        class_id[non_keep] = -1\n",
    "        class_id = class_id[all_id_sorted]\n",
    "        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]\n",
    "        # Here `pos_threshold` is a threshold for positive (non-background)\n",
    "        # predictions\n",
    "        below_min_idx = (conf < pos_threshold)\n",
    "        class_id[below_min_idx] = -1\n",
    "        conf[below_min_idx] = 1 - conf[below_min_idx]\n",
    "        pred_info = torch.cat((class_id.unsqueeze(1),\n",
    "                               conf.unsqueeze(1),\n",
    "                               predicted_bb), dim=1)\n",
    "        out.append(pred_info)\n",
    "    return torch.stack(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfff7e4",
   "metadata": {
    "origin_pos": 45
   },
   "source": [
    "Now let's **apply the above implementations\n",
    "to a concrete example with four anchor boxes**.\n",
    "For simplicity, we assume that the\n",
    "predicted offsets are all zeros.\n",
    "This means that the predicted bounding boxes are anchor boxes. \n",
    "For each class among the background, dog, and cat,\n",
    "we also define its predicted likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98db05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:58.818651Z",
     "iopub.status.busy": "2023-08-18T07:05:58.818360Z",
     "iopub.status.idle": "2023-08-18T07:05:58.823549Z",
     "shell.execute_reply": "2023-08-18T07:05:58.822717Z"
    },
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "anchors = torch.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],\n",
    "                      [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])\n",
    "offset_preds = torch.tensor([0] * anchors.numel())\n",
    "cls_probs = torch.tensor([[0] * 4,  # Predicted background likelihood\n",
    "                      [0.9, 0.8, 0.7, 0.1],  # Predicted dog likelihood\n",
    "                      [0.1, 0.2, 0.3, 0.9]])  # Predicted cat likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0b50a",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "We can **plot these predicted bounding boxes with their confidence on the image.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c253b6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:58.826799Z",
     "iopub.status.busy": "2023-08-18T07:05:58.826507Z",
     "iopub.status.idle": "2023-08-18T07:05:59.203906Z",
     "shell.execute_reply": "2023-08-18T07:05:59.203058Z"
    },
    "origin_pos": 48,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.imshow(img)\n",
    "show_bboxes(fig.axes, anchors * bbox_scale,\n",
    "            ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21753f6f",
   "metadata": {
    "origin_pos": 49
   },
   "source": [
    "Now we can invoke the `multibox_detection` function\n",
    "to perform non-maximum suppression,\n",
    "where the threshold is set to 0.5.\n",
    "Note that we add\n",
    "a dimension for examples in the tensor input.\n",
    "\n",
    "We can see that **the shape of the returned result** is\n",
    "(batch size, number of anchor boxes, 6).\n",
    "The six elements in the innermost dimension\n",
    "gives the output information for the same predicted bounding box.\n",
    "The first element is the predicted class index, which starts from 0 (0 is dog and 1 is cat). The value -1 indicates background or removal in non-maximum suppression.\n",
    "The second element is the confidence of the predicted bounding box.\n",
    "The remaining four elements are the $(x, y)$-axis coordinates of the upper-left corner and \n",
    "the lower-right corner of the predicted bounding box, respectively (range is between 0 and 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a019e03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:59.207617Z",
     "iopub.status.busy": "2023-08-18T07:05:59.207315Z",
     "iopub.status.idle": "2023-08-18T07:05:59.216131Z",
     "shell.execute_reply": "2023-08-18T07:05:59.215332Z"
    },
    "origin_pos": 51,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "output = multibox_detection(cls_probs.unsqueeze(dim=0),\n",
    "                            offset_preds.unsqueeze(dim=0),\n",
    "                            anchors.unsqueeze(dim=0),\n",
    "                            nms_threshold=0.5)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ccbe94",
   "metadata": {
    "origin_pos": 52
   },
   "source": [
    "After removing those predicted bounding boxes\n",
    "of class -1, \n",
    "we can **output the final predicted bounding box\n",
    "kept by non-maximum suppression**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e7467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:59.219942Z",
     "iopub.status.busy": "2023-08-18T07:05:59.219652Z",
     "iopub.status.idle": "2023-08-18T07:05:59.475380Z",
     "shell.execute_reply": "2023-08-18T07:05:59.474518Z"
    },
    "origin_pos": 53,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.imshow(img)\n",
    "for i in output[0].detach().numpy():\n",
    "    if i[0] == -1:\n",
    "        continue\n",
    "    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])\n",
    "    show_bboxes(fig.axes, [torch.tensor(i[2:]) * bbox_scale], label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48ba68",
   "metadata": {
    "origin_pos": 54
   },
   "source": [
    "In practice, we can remove predicted bounding boxes with lower confidence even before performing non-maximum suppression, thereby reducing computation in this algorithm.\n",
    "We may also post-process the output of non-maximum suppression, for example, by only keeping\n",
    "results with higher confidence\n",
    "in the final output.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* We generate anchor boxes with different shapes centered on each pixel of the image.\n",
    "* Intersection over union (IoU), also known as Jaccard index, measures the similarity of two bounding boxes. It is the ratio of their intersection area to their union area.\n",
    "* In a training set, we need two types of labels for each anchor box. One is the class of the object relevant to the anchor box and the other is the offset of the ground-truth bounding box relative to the anchor box.\n",
    "* During prediction, we can use non-maximum suppression (NMS) to remove similar predicted bounding boxes, thereby simplifying the output.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Change values of `sizes` and `ratios` in the `multibox_prior` function. What are the changes to the generated anchor boxes?\n",
    "1. Construct and visualize two bounding boxes with an IoU of 0.5. How do they overlap with each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af27a6ce",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
