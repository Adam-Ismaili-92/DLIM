{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714eb602",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Multiscale Object Detection\n",
    "In [Part 2](./02-anchor-student.ipynb),\n",
    "we generated multiple anchor boxes centered on each pixel of an input image. \n",
    "Essentially these anchor boxes \n",
    "represent samples of\n",
    "different regions of the image.\n",
    "However, \n",
    "we may end up with too many anchor boxes to compute\n",
    "if they are generated for *every* pixel.\n",
    "Think of a $561 \\times 728$ input image.\n",
    "If five anchor boxes \n",
    "with varying shapes\n",
    "are generated for each pixel as their center,\n",
    "over two million anchor boxes ($561 \\times 728 \\times 5$) need to be labeled and predicted on the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087f2c3",
   "metadata": {},
   "source": [
    "\n",
    "## Multiscale Anchor Boxes\n",
    "\n",
    "You may realize that\n",
    "it is not difficult to reduce anchor boxes on an image.\n",
    "For instance, we can just \n",
    "uniformly sample a small portion of pixels\n",
    "from the input image\n",
    "to generate anchor boxes centered on them.\n",
    "In addition, \n",
    "at different scales\n",
    "we can generate different numbers of anchor boxes\n",
    "of different sizes.\n",
    "Intuitively,\n",
    "smaller objects are more likely\n",
    "to appear on an image than larger ones.\n",
    "As an example,\n",
    "$1 \\times 1$, $1 \\times 2$, and $2 \\times 2$ objects \n",
    "can appear on a $2 \\times 2$ image\n",
    "in 4, 2, and 1 possible ways, respectively.\n",
    "Therefore, when using smaller anchor boxes to detect smaller objects, we can sample more regions,\n",
    "while for larger objects we can sample fewer regions.\n",
    "\n",
    "To demonstrate how to generate anchor boxes\n",
    "at multiple scales, let's read an image.\n",
    "Its height and width are 561 and 728 pixels, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98042a0",
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead9fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!test -f catdog.jpg || wget https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/catdog.jpg\n",
    "img = d2l.plt.imread('catdog.jpg')\n",
    "h, w = img.shape[:2]\n",
    "h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d62ebf",
   "metadata": {
    "origin_pos": 3
   },
   "source": [
    "Recall we call a two-dimensional array output of \n",
    "a convolutional layer a feature map.\n",
    "By defining the feature map shape,\n",
    "we can determine centers of uniformly sampled anchor boxes on any image.\n",
    "\n",
    "\n",
    "The `display_anchors` function is defined below.\n",
    "**We generate anchor boxes (`anchors`) on the feature map (`fmap`) with each unit (pixel) as the anchor box center.**\n",
    "Since the $(x, y)$-axis coordinate values\n",
    "in the anchor boxes (`anchors`) have been divided by the width and height of the feature map (`fmap`),\n",
    "these values are between 0 and 1,\n",
    "which indicate the relative positions of\n",
    "anchor boxes in the feature map.\n",
    "\n",
    "Since centers of the anchor boxes (`anchors`)\n",
    "are spread over all units on the feature map (`fmap`),\n",
    "these centers must be *uniformly* distributed\n",
    "on any input image\n",
    "in terms of their relative spatial positions.\n",
    "More concretely,\n",
    "given the width and height of the feature map `fmap_w` and `fmap_h`, respectively,\n",
    "the following function will *uniformly* sample\n",
    "pixels in `fmap_h` rows and `fmap_w` columns\n",
    "on any input image.\n",
    "Centered on these uniformly sampled pixels,\n",
    "anchor boxes of scale `s` (assuming the length of the list `s` is 1) and different aspect ratios (`ratios`)\n",
    "will be generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481508e",
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def display_anchors(fmap_w, fmap_h, s):\n",
    "    d2l.set_figsize()\n",
    "    # Values on the first two dimensions do not affect the output\n",
    "    fmap = torch.zeros((1, 10, fmap_h, fmap_w))\n",
    "    anchors = d2l.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])\n",
    "    bbox_scale = torch.tensor((w, h, w, h))\n",
    "    d2l.show_bboxes(d2l.plt.imshow(img).axes,\n",
    "                    anchors[0] * bbox_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48c1a1",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "First, let's **consider\n",
    "detection of small objects**.\n",
    "In order to make it easier to distinguish when displayed, the anchor boxes with different centers here do not overlap:\n",
    "the anchor box scale is set to 0.15\n",
    "and the height and width of the feature map are set to 4. We can see\n",
    "that the centers of the anchor boxes in 4 rows and 4 columns on the image are uniformly distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24a5b1",
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "display_anchors(fmap_w=4, fmap_h=4, s=[0.15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae96b29",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "We move on to **reduce the height and width of the feature map by half and use larger anchor boxes to detect larger objects**. When the scale is set to 0.4, \n",
    "some anchor boxes will overlap with each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f94075f",
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "display_anchors(fmap_w=2, fmap_h=2, s=[0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc225606",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "Finally, we **further reduce the height and width of the feature map by half and increase the anchor box scale to 0.8**. Now the center of the anchor box is the center of the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd9a6c",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "display_anchors(fmap_w=1, fmap_h=1, s=[0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6c6c1",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## Multiscale Detection\n",
    "\n",
    "\n",
    "Since we have generated multiscale anchor boxes,\n",
    "we will use them to detect objects of various sizes\n",
    "at different scales.\n",
    "In the following\n",
    "we introduce a CNN-based multiscale object detection\n",
    "method that we will implement\n",
    "in [Part 5](./05-ssd-student.ipynb).\n",
    "\n",
    "At some scale,\n",
    "say that we have $c$ feature maps of shape $h \\times w$.\n",
    "Using the method previous section,\n",
    "we generate $hw$ sets of anchor boxes,\n",
    "where each set has $a$ anchor boxes with the same center.\n",
    "For example, \n",
    "at the first scale in the experiments in the previous section,\n",
    "given ten (number of channels) $4 \\times 4$ feature maps,\n",
    "we generated 16 sets of anchor boxes,\n",
    "where each set contains 3 anchor boxes with the same center.\n",
    "Next, each anchor box is labeled with\n",
    "the class and offset based on ground-truth bounding boxes. At the current scale, the object detection model needs to predict the classes and offsets of $hw$ sets of anchor boxes on the input image, where different sets have different centers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e9f15",
   "metadata": {},
   "source": [
    "\n",
    "Assume that the $c$ feature maps here\n",
    "are the intermediate outputs obtained\n",
    "by the CNN forward propagation based on the input image. Since there are $hw$ different spatial positions on each feature map,\n",
    "the same spatial position can be \n",
    "thought of as having $c$ units.\n",
    "We say that these $c$ units at the **same spatial position\n",
    "of the feature maps**\n",
    "have the same **receptive field** on the input image:\n",
    "they represent the input image information\n",
    "in the same receptive field.\n",
    "Therefore, we can transform the $c$ units\n",
    "of the feature maps at the same spatial position\n",
    "into the\n",
    "classes and offsets of the $a$ anchor boxes\n",
    "generated using this spatial position.\n",
    "In essence,\n",
    "we use the information of the input image in a certain receptive field\n",
    "to predict the classes and offsets of the anchor boxes\n",
    "that are\n",
    "close to that receptive field\n",
    "on the input image.\n",
    "\n",
    "\n",
    "When the feature maps at different layers\n",
    "have varying-size receptive fields on the input image, they can be used to detect objects of different sizes.\n",
    "For example, we can design a neural network where\n",
    "units of feature maps that are closer to the output layer\n",
    "have wider receptive fields,\n",
    "so they can detect larger objects from the input image.\n",
    "\n",
    "In a nutshell, we can leverage\n",
    "layerwise representations of images at multiple levels\n",
    "by deep neural networks\n",
    "for multiscale object detection.\n",
    "We will show how this works through a concrete example\n",
    "in [Part 5](./05-ssd-student.ipynb).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4377856",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "* At multiple scales, we can generate anchor boxes with different sizes to detect objects with different sizes.\n",
    "* By defining the shape of feature maps, we can determine centers of uniformly sampled anchor boxes on any image.\n",
    "* We use the information of the input image in a certain receptive field to predict the classes and offsets of the anchor boxes that are close to that receptive field on the input image.\n",
    "* Through deep learning, we can leverage its layerwise representations of images at multiple levels for multiscale object detection.\n",
    "\n",
    "\n",
    "## Questions\n",
    "\n",
    "> Given a feature map variable with shape $1 \\times c \\times h \\times w$, where $c$, $h$, and $w$ are the number of channels, height, and width of the feature maps, respectively. How can you transform this variable into the classes and offsets of anchor boxes? What is the shape of the output?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd5cdae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
